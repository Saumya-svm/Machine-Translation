{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To-Do\n",
        "- [ ] Create Separate Functions for each task\n",
        "- [ ] Evaluation of our model\n",
        "- [ ] How to generalize the model?\n",
        "- [ ] More about preprocessing\n",
        "- [ ] Improve Handling of rare words"
      ],
      "metadata": {
        "id": "yBQU6qwn00-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iilCsyZm8x0t",
        "outputId": "608ae3f4-7a3f-4883-f1c7-37f87c664345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras-preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras-preprocessing) (1.15.0)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-text==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxYBi-0lCV09",
        "outputId": "a21f1696-f9fb-492e-e0ae-b5d06c7f67b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text==2.11.0) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.1.21)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (15.0.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.51.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, SimpleRNN, RepeatVector, TimeDistributed"
      ],
      "metadata": {
        "id": "JUuV3MtyBIVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SEMBekqmA2Mn",
        "outputId": "e6f356f2-7e0a-417d-d18d-74b5ac1acd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.11.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English to Spanish"
      ],
      "metadata": {
        "id": "72epXW2bfogS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtxLzvJZ_3mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd4c135-f989-4216-dae5-b20d0aac4d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  context = np.array([context for target, context in pairs])\n",
        "  target = np.array([target for target, context in pairs])\n",
        "\n",
        "  return target, context\n",
        "\n",
        "data = load_data(path_to_file)"
      ],
      "metadata": {
        "id": "uvC24IqtEqND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array(data[0][:10000])\n",
        "outputs = np.array(data[1][:10000])"
      ],
      "metadata": {
        "id": "71w7mihMFf6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = tf.keras.layers.TextVectorization(standardize=\"lower_and_strip_punctuation\", output_sequence_length=src)\n",
        "# text_dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "# vectorizer.adapt(text_dataset)"
      ],
      "metadata": {
        "id": "25GDof7ZH_cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # dump(vectorizer,open('drive/MyDrive/Machine Translation/vectroizer.pkl','wb'))\n",
        "# import pickle\n",
        "# pickle.dump({'config': vectorizer.get_config(),\n",
        "#              'weights': vectorizer.get_weights()}\n",
        "#             , open(\"drive/MyDrive/Machine Translation/vectorizer.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "7gXUv2H4u_fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# saved = pickle.load(open('drive/MyDrive/Machine Translation/vectorizer.pkl','rb'))\n",
        "# vectorizer = tf.keras.layers.TextVectorization.from_config(saved['config'])\n",
        "\n",
        "# vectorizer.adapt(tf.data.Dataset.from_tensor_slices(['random']))\n",
        "# vectorizer.set_weights(saved['weights'])"
      ],
      "metadata": {
        "id": "22735waO0qIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spanish_vectorizer = tf.keras.layers.TextVectorization(standardize=\"lower_and_strip_punctuation\", output_sequence_length=tar)\n",
        "# text_dataset = tf.data.Dataset.from_tensor_slices(outputs)\n",
        "# spanish_vectorizer.adapt(text_dataset)"
      ],
      "metadata": {
        "id": "zQFwEGKz7E0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': spanish_vectorizer.get_config(),\n",
        "#              'weights': spanish_vectorizer.get_weights()}\n",
        "#             , open(\"drive/MyDrive/Machine Translation/spanish_vectorizer.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "qrqQQM-8Fldh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saved = pickle.load(open('drive/MyDrive/Machine Translation/spanish_vectorizer.pkl','rb'))\n",
        "# spanish_vectorizer = tf.keras.layers.TextVectorization.from_config(saved['config'])\n",
        "\n",
        "# spanish_vectorizer.adapt(tf.data.Dataset.from_tensor_slices(['random']))\n",
        "# spanish_vectorizer.set_weights(saved['weights'])"
      ],
      "metadata": {
        "id": "NG3zJ4o9FphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_X = vectorizer(inputs[indices])\n",
        "# train_Y = spanish_vectorizer(outputs[indices])\n",
        "# # model.fit(train_X, train_Y)"
      ],
      "metadata": {
        "id": "rkTGUT6RrsXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokenizer(source, target, num_words=5000, lower=True):\n",
        "  src_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,lower=lower)\n",
        "  tar_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,lower=lower)\n",
        "\n",
        "  src_tokenizer.fit_on_texts(source)\n",
        "  tar_tokenizer.fit_on_texts(target)\n",
        "  return src_tokenizer, tar_tokenizer"
      ],
      "metadata": {
        "id": "wSPUXMU7DW2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer,spa_tokenizer = create_tokenizer(inputs, outputs)"
      ],
      "metadata": {
        "id": "hSY-Xcxc1tKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(source, target):\n",
        "  source = eng_tokenizer.texts_to_sequences(inputs)\n",
        "  source = tf.keras.preprocessing.sequence.pad_sequences(source, maxlen=8, padding='post')\n",
        "\n",
        "  target = spa_tokenizer.texts_to_sequences(outputs)\n",
        "  target = tf.keras.preprocessing.sequence.pad_sequences(target, maxlen=8, padding='post')\n",
        "  return source, target\n",
        "\n",
        "def split_data(source, target, train_split=0.8):\n",
        "  size = inputs.shape[0]\n",
        "  indices = np.random.randint(0, size, size=(int(size*train_split),))\n",
        "\n",
        "  trainX = source[indices]\n",
        "  trainY = target[indices]\n",
        "\n",
        "  mask = np.ones(size,bool)\n",
        "  mask[indices] = False\n",
        "  testX = source[mask]\n",
        "  testY = target[mask]\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "source, target = transform_data(inputs, outputs)\n",
        "trainX, trainY, testX, testY = split_data(source, target, 0.8)"
      ],
      "metadata": {
        "id": "isM-WspK3EE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        " ylist = list()\n",
        " for sequence in sequences:\n",
        "  encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "  ylist.append(encoded)\n",
        " y = np.array(ylist)\n",
        " y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        " return y\n",
        "\n",
        "trainY = encode_output(trainY, len(spa_tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "yiyX-srWESTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = len(spa_tokenizer.word_index)+1\n",
        "src_vocab = len(eng_tokenizer.word_index)+1\n",
        "src = max(len(line.split()) for line in inputs)\n",
        "tar = max(len(line.split()) for line in outputs)"
      ],
      "metadata": {
        "id": "e31oZY9SlTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = Embedding(1000, 16, input_length=8)\n",
        "layer2 = LSTM(256)\n",
        "model = Sequential()\n",
        "model.add(layer1)\n",
        "model.add(layer2)\n",
        "\n",
        "# we want to replicate the context vector for each time step\n",
        "model.add(RepeatVector(8))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "# converting decoder output to our desired sequence format\n",
        "model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "jTb_utCwF0Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf3QbyPwdvkI",
        "outputId": "0cd1e4ec-7780-4983-88cf-601377aa83f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 8, 16)             16000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               279552    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 8, 256)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 8, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 8, 4961)          1274977   \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,095,841\n",
            "Trainable params: 2,095,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainX, trainY, epochs=30, batch_size=64)"
      ],
      "metadata": {
        "id": "yyIGyKKN5EcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902460ec-be68-4433-d949-16e097684c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "125/125 [==============================] - 14s 45ms/step - loss: 3.3985\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 2.5767\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 2.5141\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.4538\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.3953\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.3504\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 2.3107\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 2.2712\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.2328\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 2.2040\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.1764\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 2.1483\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 2.1246\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 2.1042\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 2.0794\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 2.0576\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.0340\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 2.0029\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9632\n",
            "Epoch 20/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.9235\n",
            "Epoch 21/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 1.8870\n",
            "Epoch 22/30\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 1.8486\n",
            "Epoch 23/30\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 1.8148\n",
            "Epoch 24/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.7757\n",
            "Epoch 25/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.7337\n",
            "Epoch 26/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.6983\n",
            "Epoch 27/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.6602\n",
            "Epoch 28/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.6228\n",
            "Epoch 29/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.5843\n",
            "Epoch 30/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.5502\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc6b78e8dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, input, source_tokenizer, tar_token_to_word):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  input - a string in the source language\n",
        "  \"\"\"\n",
        "  # tokenized = source_tokenizer.texts_to_sequences(input)\n",
        "  # test = tf.keras.preprocessing.sequence.pad_sequences(tokenized, maxlen=8, padding='post')\n",
        "  prediction = model.predict(input)\n",
        "\n",
        "  output = [np.argmax(vector) for vector in prediction[0]]\n",
        "\n",
        "  output_list = []\n",
        "  for i in output:\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      output_list.append(tar_token_to_word[i])\n",
        "\n",
        "  output_sentence = ' '.join(output_list)\n",
        "  return output_sentence\n",
        "\n",
        "eng_word_to_token = eng_tokenizer.word_index\n",
        "eng_token_to_word = {token:word for word, token in eng_word_to_token.items()}\n",
        "\n",
        "spa_word_to_token = spa_tokenizer.word_index\n",
        "spa_token_to_word = {token:word for word, token in spa_word_to_token.items()}"
      ],
      "metadata": {
        "id": "1Aq8jC7zC-2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model, trainX[1:3], eng_tokenizer, spa_token_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0yrPlRfpbr76",
        "outputId": "bb90b574-4845-414b-c2aa-1ca868240f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 640ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me llamé'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eng to Hin (Incomplete)"
      ],
      "metadata": {
        "id": "zfCz8EmQfhgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hin.txt') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "jQVzihcHevg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-TPwGoUoGOz",
        "outputId": "a7976530-9e92-43e0-8985-1f8f2a15bf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2909"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(lines):\n",
        "  inputs = []\n",
        "  outputs = []\n",
        "\n",
        "  for i in range(len(lines)):\n",
        "    src, target, _ = lines[i].split(\"\\t\")\n",
        "    inputs.append(src)\n",
        "    outputs.append(target)\n",
        "  \n",
        "  inputs = np.array(inputs)\n",
        "  outputs = np.array(outputs)\n",
        "  return inputs, outputs\n",
        "\n",
        "inputs, outputs = prepare_data(lines)"
      ],
      "metadata": {
        "id": "Fesx1GluoS6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape, outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXrcmh1ioykU",
        "outputId": "cbe1284b-cf1a-4a5a-f1a4-698237d85160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2909,), (2909,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,lower=True)\n",
        "hin_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,lower=True)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(inputs)\n",
        "hin_tokenizer.fit_on_texts(outputs)\n",
        "\n",
        "trainX = eng_tokenizer.texts_to_sequences(inputs)\n",
        "trainX = tf.keras.preprocessing.sequence.pad_sequences(trainX, maxlen=8, padding='post')\n",
        "\n",
        "trainY = hin_tokenizer.texts_to_sequences(outputs)\n",
        "trainY = tf.keras.preprocessing.sequence.pad_sequences(trainY, maxlen=8, padding='post')\n",
        "\n",
        "def encode_output(sequences, vocab_size):\n",
        " ylist = list()\n",
        " for sequence in sequences:\n",
        "  encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "  ylist.append(encoded)\n",
        " y = np.array(ylist)\n",
        " y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        " return y\n",
        "\n",
        "trainY = encode_output(trainY[:9000], len(hin_tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "ZfUtn7k9q_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hin_tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj0VzOpStLf-",
        "outputId": "469c667f-6ac5-4f5a-99ad-d9e688e0d11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3012"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = len(hin_tokenizer.word_index)+1\n",
        "src_vocab = len(eng_tokenizer.word_index)+1\n",
        "src = max(len(line.split()) for line in inputs)\n",
        "tar = max(len(line.split()) for line in outputs)"
      ],
      "metadata": {
        "id": "IIrHxw7AsNsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = Embedding(src_vocab, 16, input_length=8)\n",
        "layer2 = LSTM(256)\n",
        "model = Sequential()\n",
        "model.add(layer1)\n",
        "model.add(layer2)\n",
        "\n",
        "# we want to replicate the context vector for each time step\n",
        "model.add(RepeatVector(8))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "# converting decoder output to our desired sequence format\n",
        "model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "0Naul2XIsl5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPyy-C9vEUes",
        "outputId": "7b6a1aea-d46d-4207-a77f-9a9865cf46af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 8, 16)             38336     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               279552    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 8, 256)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 8, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 8, 3013)          774341    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,617,541\n",
            "Trainable params: 1,617,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGZslmep0CUE",
        "outputId": "a020cda0-2fb7-4cd9-b2c4-1cf16fa81c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2909, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons==0.16.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD3pmD6OdmLf",
        "outputId": "d7a4d532-7af5-4a6d-e39f-9210d09a9610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons==0.16.1\n",
            "  Downloading tensorflow_addons-0.16.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons==0.16.1) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf4MriT5oVTY",
        "outputId": "7896209d-35e9-4e78-feef-ae070f5e9ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bi_model = Sequential()\n",
        "bi_model.add(Embedding(src_vocab, 16, input_length=8))\n",
        "bi_model.add(tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True), input_shape=(8,16)))\n",
        "bi_model.add(tf.keras.layers.Attention(256))\n",
        "bi_model.add(LSTM(256, return_sequences=True))\n",
        "bi_model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "bi_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "bi_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "TM8GKc6tDY5o",
        "outputId": "faf74ed7-5c7f-4f61-d147-d5ca329c338d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-300208a6f737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/attention/base_dense_attention.py\u001b[0m in \u001b[0;36m_validate_call_args\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0;34mf\"{class_name} layer must be called on a list of inputs, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"namely [query, value] or [query, value, key]. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention\" (type Attention).\n\nAttention layer must be called on a list of inputs, namely [query, value] or [query, value, key]. Received: Tensor(\"Placeholder:0\", shape=(None, 8, 512), dtype=float32).\n\nCall arguments received by layer \"attention\" (type Attention):\n  • inputs=tf.Tensor(shape=(None, 8, 512), dtype=float32)\n  • mask=None\n  • training=None\n  • return_attention_scores=False\n  • use_causal_mask=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_nQjPDQ8s5z",
        "outputId": "b18f8e15-3be6-42cc-919d-55c867c656db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2909, 8, 3013)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bi_model.fit(trainX, trainY, epochs=250, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxSG_i6l5DbW",
        "outputId": "c3a613bb-77dd-4a99-f76a-1ac8b0cb546b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.8828\n",
            "Epoch 2/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.8536\n",
            "Epoch 3/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.8345\n",
            "Epoch 4/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.8100\n",
            "Epoch 5/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.8015\n",
            "Epoch 6/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.7893\n",
            "Epoch 7/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.7810\n",
            "Epoch 8/250\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.7724\n",
            "Epoch 9/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.7632\n",
            "Epoch 10/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.7563\n",
            "Epoch 11/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.7511\n",
            "Epoch 12/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.7441\n",
            "Epoch 13/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.7334\n",
            "Epoch 14/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.7224\n",
            "Epoch 15/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.7141\n",
            "Epoch 16/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.7075\n",
            "Epoch 17/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.7040\n",
            "Epoch 18/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6946\n",
            "Epoch 19/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6864\n",
            "Epoch 20/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6802\n",
            "Epoch 21/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.6768\n",
            "Epoch 22/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6734\n",
            "Epoch 23/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6660\n",
            "Epoch 24/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6595\n",
            "Epoch 25/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.6540\n",
            "Epoch 26/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6545\n",
            "Epoch 27/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6529\n",
            "Epoch 28/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.6588\n",
            "Epoch 29/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.6932\n",
            "Epoch 30/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.7492\n",
            "Epoch 31/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.7642\n",
            "Epoch 32/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.7235\n",
            "Epoch 33/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6821\n",
            "Epoch 34/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.6388\n",
            "Epoch 35/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.6076\n",
            "Epoch 36/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5917\n",
            "Epoch 37/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5810\n",
            "Epoch 38/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5735\n",
            "Epoch 39/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5682\n",
            "Epoch 40/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5615\n",
            "Epoch 41/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5559\n",
            "Epoch 42/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5509\n",
            "Epoch 43/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.5491\n",
            "Epoch 44/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5438\n",
            "Epoch 45/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5405\n",
            "Epoch 46/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5345\n",
            "Epoch 47/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5321\n",
            "Epoch 48/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.5269\n",
            "Epoch 49/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5221\n",
            "Epoch 50/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.5169\n",
            "Epoch 51/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.5116\n",
            "Epoch 52/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.5046\n",
            "Epoch 53/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4991\n",
            "Epoch 54/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4940\n",
            "Epoch 55/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4913\n",
            "Epoch 56/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4951\n",
            "Epoch 57/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4907\n",
            "Epoch 58/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4846\n",
            "Epoch 59/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4769\n",
            "Epoch 60/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4773\n",
            "Epoch 61/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4693\n",
            "Epoch 62/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4636\n",
            "Epoch 63/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4559\n",
            "Epoch 64/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4505\n",
            "Epoch 65/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4450\n",
            "Epoch 66/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4525\n",
            "Epoch 67/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4586\n",
            "Epoch 68/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4544\n",
            "Epoch 69/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4466\n",
            "Epoch 70/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4400\n",
            "Epoch 71/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4325\n",
            "Epoch 72/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4261\n",
            "Epoch 73/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4267\n",
            "Epoch 74/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.4356\n",
            "Epoch 75/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.4524\n",
            "Epoch 76/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4859\n",
            "Epoch 77/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.5136\n",
            "Epoch 78/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4816\n",
            "Epoch 79/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4390\n",
            "Epoch 80/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.4064\n",
            "Epoch 81/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3899\n",
            "Epoch 82/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3811\n",
            "Epoch 83/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3743\n",
            "Epoch 84/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3702\n",
            "Epoch 85/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3645\n",
            "Epoch 86/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3617\n",
            "Epoch 87/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3580\n",
            "Epoch 88/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3544\n",
            "Epoch 89/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3519\n",
            "Epoch 90/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3485\n",
            "Epoch 91/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3452\n",
            "Epoch 92/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3438\n",
            "Epoch 93/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3395\n",
            "Epoch 94/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3357\n",
            "Epoch 95/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3331\n",
            "Epoch 96/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3342\n",
            "Epoch 97/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.3323\n",
            "Epoch 98/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3304\n",
            "Epoch 99/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3272\n",
            "Epoch 100/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3263\n",
            "Epoch 101/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3195\n",
            "Epoch 102/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3139\n",
            "Epoch 103/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3098\n",
            "Epoch 104/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3080\n",
            "Epoch 105/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3087\n",
            "Epoch 106/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3065\n",
            "Epoch 107/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3012\n",
            "Epoch 108/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2991\n",
            "Epoch 109/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3013\n",
            "Epoch 110/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3149\n",
            "Epoch 111/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3592\n",
            "Epoch 112/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4623\n",
            "Epoch 113/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4898\n",
            "Epoch 114/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.4055\n",
            "Epoch 115/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3420\n",
            "Epoch 116/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.3067\n",
            "Epoch 117/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2857\n",
            "Epoch 118/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2763\n",
            "Epoch 119/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2690\n",
            "Epoch 120/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.2635\n",
            "Epoch 121/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2600\n",
            "Epoch 122/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2566\n",
            "Epoch 123/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.2539\n",
            "Epoch 124/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2507\n",
            "Epoch 125/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2485\n",
            "Epoch 126/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2462\n",
            "Epoch 127/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2449\n",
            "Epoch 128/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2420\n",
            "Epoch 129/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2389\n",
            "Epoch 130/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2383\n",
            "Epoch 131/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.2354\n",
            "Epoch 132/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2329\n",
            "Epoch 133/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.2323\n",
            "Epoch 134/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2279\n",
            "Epoch 135/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2260\n",
            "Epoch 136/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2228\n",
            "Epoch 137/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.2215\n",
            "Epoch 138/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2218\n",
            "Epoch 139/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.2220\n",
            "Epoch 140/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2197\n",
            "Epoch 141/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2176\n",
            "Epoch 142/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2193\n",
            "Epoch 143/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.2151\n",
            "Epoch 144/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2105\n",
            "Epoch 145/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2094\n",
            "Epoch 146/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.2088\n",
            "Epoch 147/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.2122\n",
            "Epoch 148/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2052\n",
            "Epoch 149/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.2016\n",
            "Epoch 150/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1962\n",
            "Epoch 151/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1937\n",
            "Epoch 152/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1913\n",
            "Epoch 153/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1916\n",
            "Epoch 154/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1890\n",
            "Epoch 155/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1882\n",
            "Epoch 156/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1931\n",
            "Epoch 157/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1989\n",
            "Epoch 158/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2021\n",
            "Epoch 159/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2205\n",
            "Epoch 160/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2604\n",
            "Epoch 161/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.3094\n",
            "Epoch 162/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2994\n",
            "Epoch 163/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2520\n",
            "Epoch 164/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.2145\n",
            "Epoch 165/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.1926\n",
            "Epoch 166/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1774\n",
            "Epoch 167/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1698\n",
            "Epoch 168/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1659\n",
            "Epoch 169/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1628\n",
            "Epoch 170/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1601\n",
            "Epoch 171/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1596\n",
            "Epoch 172/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1572\n",
            "Epoch 173/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1551\n",
            "Epoch 174/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1531\n",
            "Epoch 175/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1518\n",
            "Epoch 176/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1507\n",
            "Epoch 177/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1480\n",
            "Epoch 178/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1473\n",
            "Epoch 179/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1449\n",
            "Epoch 180/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1441\n",
            "Epoch 181/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1424\n",
            "Epoch 182/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1415\n",
            "Epoch 183/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1406\n",
            "Epoch 184/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1396\n",
            "Epoch 185/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1386\n",
            "Epoch 186/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1373\n",
            "Epoch 187/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.1353\n",
            "Epoch 188/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1338\n",
            "Epoch 189/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1329\n",
            "Epoch 190/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1318\n",
            "Epoch 191/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1301\n",
            "Epoch 192/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1301\n",
            "Epoch 193/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1291\n",
            "Epoch 194/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1281\n",
            "Epoch 195/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1270\n",
            "Epoch 196/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1248\n",
            "Epoch 197/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1237\n",
            "Epoch 198/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1222\n",
            "Epoch 199/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1207\n",
            "Epoch 200/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1200\n",
            "Epoch 201/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1188\n",
            "Epoch 202/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1186\n",
            "Epoch 203/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1171\n",
            "Epoch 204/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1162\n",
            "Epoch 205/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1161\n",
            "Epoch 206/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1150\n",
            "Epoch 207/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1155\n",
            "Epoch 208/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1144\n",
            "Epoch 209/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1200\n",
            "Epoch 210/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.1592\n",
            "Epoch 211/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3544\n",
            "Epoch 212/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4101\n",
            "Epoch 213/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2775\n",
            "Epoch 214/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1781\n",
            "Epoch 215/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1319\n",
            "Epoch 216/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1165\n",
            "Epoch 217/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.1097\n",
            "Epoch 218/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1064\n",
            "Epoch 219/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1048\n",
            "Epoch 220/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1022\n",
            "Epoch 221/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.1017\n",
            "Epoch 222/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0995\n",
            "Epoch 223/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.0983\n",
            "Epoch 224/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0970\n",
            "Epoch 225/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0961\n",
            "Epoch 226/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.0951\n",
            "Epoch 227/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0950\n",
            "Epoch 228/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0938\n",
            "Epoch 229/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0927\n",
            "Epoch 230/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0917\n",
            "Epoch 231/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.0912\n",
            "Epoch 232/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0906\n",
            "Epoch 233/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0898\n",
            "Epoch 234/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0894\n",
            "Epoch 235/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0887\n",
            "Epoch 236/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0870\n",
            "Epoch 237/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0873\n",
            "Epoch 238/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0869\n",
            "Epoch 239/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0862\n",
            "Epoch 240/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0850\n",
            "Epoch 241/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0849\n",
            "Epoch 242/250\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 0.0836\n",
            "Epoch 243/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0830\n",
            "Epoch 244/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0825\n",
            "Epoch 245/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0817\n",
            "Epoch 246/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0826\n",
            "Epoch 247/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0826\n",
            "Epoch 248/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0820\n",
            "Epoch 249/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 0.0820\n",
            "Epoch 250/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.0824\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6462434130>"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bi_model.predict(trainX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxw87ktU39xQ",
        "outputId": "b9744386-b3da-4636-d26e-999c9b325a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 [==============================] - 3s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQkzAIgS4MYq",
        "outputId": "85bfb144-f283-4d23-cfd0-fd67aa8a3c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2909, 8, 3013)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[np.argmax(vector) for vector in predictions[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vSD0kZj9Rbn",
        "outputId": "84f2c20e-487b-41ba-9b46-7c607cabe4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[775, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainX, trainY, epochs=250, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrscudPjsoBN",
        "outputId": "20e83898-cf6e-4423-9c60-5ff3d38d6639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "46/46 [==============================] - 16s 124ms/step - loss: 6.3958\n",
            "Epoch 2/250\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 5.4829\n",
            "Epoch 3/250\n",
            "46/46 [==============================] - 1s 22ms/step - loss: 5.2873\n",
            "Epoch 4/250\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.1674\n",
            "Epoch 5/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 5.0356\n",
            "Epoch 6/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.9334\n",
            "Epoch 7/250\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 4.8706\n",
            "Epoch 8/250\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 4.8168\n",
            "Epoch 9/250\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 4.7599\n",
            "Epoch 10/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.7259\n",
            "Epoch 11/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.6908\n",
            "Epoch 12/250\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 4.6545\n",
            "Epoch 13/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.6053\n",
            "Epoch 14/250\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 4.5678\n",
            "Epoch 15/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.5361\n",
            "Epoch 16/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.5038\n",
            "Epoch 17/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.4860\n",
            "Epoch 18/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.4568\n",
            "Epoch 19/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.4497\n",
            "Epoch 20/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.4178\n",
            "Epoch 21/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.3878\n",
            "Epoch 22/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.3636\n",
            "Epoch 23/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.3465\n",
            "Epoch 24/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 4.3344\n",
            "Epoch 25/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 4.3307\n",
            "Epoch 26/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.3128\n",
            "Epoch 27/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.2893\n",
            "Epoch 28/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.2576\n",
            "Epoch 29/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 4.2315\n",
            "Epoch 30/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.2224\n",
            "Epoch 31/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.2120\n",
            "Epoch 32/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.1854\n",
            "Epoch 33/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.1620\n",
            "Epoch 34/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.1378\n",
            "Epoch 35/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 4.1176\n",
            "Epoch 36/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 4.0942\n",
            "Epoch 37/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 4.0731\n",
            "Epoch 38/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 4.0726\n",
            "Epoch 39/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.0454\n",
            "Epoch 40/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 4.0097\n",
            "Epoch 41/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.9787\n",
            "Epoch 42/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.9530\n",
            "Epoch 43/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.9350\n",
            "Epoch 44/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.9218\n",
            "Epoch 45/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.8900\n",
            "Epoch 46/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.8684\n",
            "Epoch 47/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.8425\n",
            "Epoch 48/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.8223\n",
            "Epoch 49/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.7937\n",
            "Epoch 50/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.7633\n",
            "Epoch 51/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.7444\n",
            "Epoch 52/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.7207\n",
            "Epoch 53/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.7036\n",
            "Epoch 54/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.6792\n",
            "Epoch 55/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.6526\n",
            "Epoch 56/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.6288\n",
            "Epoch 57/250\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 3.6205\n",
            "Epoch 58/250\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 3.5827\n",
            "Epoch 59/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.5455\n",
            "Epoch 60/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 3.5208\n",
            "Epoch 61/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.4930\n",
            "Epoch 62/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.4612\n",
            "Epoch 63/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.4455\n",
            "Epoch 64/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.4118\n",
            "Epoch 65/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.3760\n",
            "Epoch 66/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.3545\n",
            "Epoch 67/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.3298\n",
            "Epoch 68/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.3082\n",
            "Epoch 69/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.2768\n",
            "Epoch 70/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.2467\n",
            "Epoch 71/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.2183\n",
            "Epoch 72/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.2051\n",
            "Epoch 73/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.1726\n",
            "Epoch 74/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.1507\n",
            "Epoch 75/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.1147\n",
            "Epoch 76/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 3.1013\n",
            "Epoch 77/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.0711\n",
            "Epoch 78/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.0440\n",
            "Epoch 79/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.0426\n",
            "Epoch 80/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.0092\n",
            "Epoch 81/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.9737\n",
            "Epoch 82/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 2.9384\n",
            "Epoch 83/250\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 2.9059\n",
            "Epoch 84/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 2.8819\n",
            "Epoch 85/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.8772\n",
            "Epoch 86/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.8455\n",
            "Epoch 87/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.8218\n",
            "Epoch 88/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.8021\n",
            "Epoch 89/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.7711\n",
            "Epoch 90/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.7469\n",
            "Epoch 91/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.7284\n",
            "Epoch 92/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.6987\n",
            "Epoch 93/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.6747\n",
            "Epoch 94/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 2.6598\n",
            "Epoch 95/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 2.6184\n",
            "Epoch 96/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.5980\n",
            "Epoch 97/250\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 2.5717\n",
            "Epoch 98/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.5491\n",
            "Epoch 99/250\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 2.5457\n",
            "Epoch 100/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.5133\n",
            "Epoch 101/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.4873\n",
            "Epoch 102/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.4581\n",
            "Epoch 103/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.4328\n",
            "Epoch 104/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.4189\n",
            "Epoch 105/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 2.3891\n",
            "Epoch 106/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 2.3691\n",
            "Epoch 107/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 2.3595\n",
            "Epoch 108/250\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 2.3308\n",
            "Epoch 109/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.3030\n",
            "Epoch 110/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.2732\n",
            "Epoch 111/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.2445\n",
            "Epoch 112/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.2300\n",
            "Epoch 113/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 2.2079\n",
            "Epoch 114/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.1876\n",
            "Epoch 115/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 2.1816\n",
            "Epoch 116/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.1601\n",
            "Epoch 117/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.1362\n",
            "Epoch 118/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.1212\n",
            "Epoch 119/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.0982\n",
            "Epoch 120/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.0757\n",
            "Epoch 121/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 2.0430\n",
            "Epoch 122/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 2.0165\n",
            "Epoch 123/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.9992\n",
            "Epoch 124/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.9828\n",
            "Epoch 125/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.9564\n",
            "Epoch 126/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.9383\n",
            "Epoch 127/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.9295\n",
            "Epoch 128/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.8943\n",
            "Epoch 129/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.8679\n",
            "Epoch 130/250\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 1.8476\n",
            "Epoch 131/250\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 1.8438\n",
            "Epoch 132/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.8638\n",
            "Epoch 133/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.8499\n",
            "Epoch 134/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.8095\n",
            "Epoch 135/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.7854\n",
            "Epoch 136/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.7501\n",
            "Epoch 137/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.7179\n",
            "Epoch 138/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.6835\n",
            "Epoch 139/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.6629\n",
            "Epoch 140/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.6427\n",
            "Epoch 141/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.6417\n",
            "Epoch 142/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.6453\n",
            "Epoch 143/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.6270\n",
            "Epoch 144/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.5981\n",
            "Epoch 145/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.5830\n",
            "Epoch 146/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.5627\n",
            "Epoch 147/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.5407\n",
            "Epoch 148/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.5331\n",
            "Epoch 149/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.5208\n",
            "Epoch 150/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.4823\n",
            "Epoch 151/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.4552\n",
            "Epoch 152/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.4385\n",
            "Epoch 153/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.4286\n",
            "Epoch 154/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.4213\n",
            "Epoch 155/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.4034\n",
            "Epoch 156/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.3828\n",
            "Epoch 157/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.3663\n",
            "Epoch 158/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.3599\n",
            "Epoch 159/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.3383\n",
            "Epoch 160/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.3257\n",
            "Epoch 161/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3183\n",
            "Epoch 162/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.3045\n",
            "Epoch 163/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.2716\n",
            "Epoch 164/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.2518\n",
            "Epoch 165/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.2338\n",
            "Epoch 166/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.2151\n",
            "Epoch 167/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.2031\n",
            "Epoch 168/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1827\n",
            "Epoch 169/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1626\n",
            "Epoch 170/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1547\n",
            "Epoch 171/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1431\n",
            "Epoch 172/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1265\n",
            "Epoch 173/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.1136\n",
            "Epoch 174/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.1083\n",
            "Epoch 175/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1140\n",
            "Epoch 176/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.0996\n",
            "Epoch 177/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.0766\n",
            "Epoch 178/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.0384\n",
            "Epoch 179/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.0136\n",
            "Epoch 180/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9986\n",
            "Epoch 181/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.9997\n",
            "Epoch 182/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.9893\n",
            "Epoch 183/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.9804\n",
            "Epoch 184/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.9644\n",
            "Epoch 185/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9589\n",
            "Epoch 186/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9573\n",
            "Epoch 187/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9334\n",
            "Epoch 188/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9114\n",
            "Epoch 189/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.9105\n",
            "Epoch 190/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.9164\n",
            "Epoch 191/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9181\n",
            "Epoch 192/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.9090\n",
            "Epoch 193/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.8802\n",
            "Epoch 194/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.8423\n",
            "Epoch 195/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.8092\n",
            "Epoch 196/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.7898\n",
            "Epoch 197/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.7778\n",
            "Epoch 198/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.7710\n",
            "Epoch 199/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.7589\n",
            "Epoch 200/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.7476\n",
            "Epoch 201/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.7311\n",
            "Epoch 202/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.7182\n",
            "Epoch 203/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.7084\n",
            "Epoch 204/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.7026\n",
            "Epoch 205/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6911\n",
            "Epoch 206/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.6851\n",
            "Epoch 207/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.6862\n",
            "Epoch 208/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.7005\n",
            "Epoch 209/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.7126\n",
            "Epoch 210/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.7141\n",
            "Epoch 211/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6922\n",
            "Epoch 212/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.6958\n",
            "Epoch 213/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6665\n",
            "Epoch 214/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.6354\n",
            "Epoch 215/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6128\n",
            "Epoch 216/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5995\n",
            "Epoch 217/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5748\n",
            "Epoch 218/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5525\n",
            "Epoch 219/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5411\n",
            "Epoch 220/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5312\n",
            "Epoch 221/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5176\n",
            "Epoch 222/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5062\n",
            "Epoch 223/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4981\n",
            "Epoch 224/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4927\n",
            "Epoch 225/250\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.4829\n",
            "Epoch 226/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4782\n",
            "Epoch 227/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4701\n",
            "Epoch 228/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4619\n",
            "Epoch 229/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.4597\n",
            "Epoch 230/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4689\n",
            "Epoch 231/250\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.4710\n",
            "Epoch 232/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4650\n",
            "Epoch 233/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.4686\n",
            "Epoch 234/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4750\n",
            "Epoch 235/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5754\n",
            "Epoch 236/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6723\n",
            "Epoch 237/250\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.6819\n",
            "Epoch 238/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.6476\n",
            "Epoch 239/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5073\n",
            "Epoch 240/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.4222\n",
            "Epoch 241/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3813\n",
            "Epoch 242/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3595\n",
            "Epoch 243/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3476\n",
            "Epoch 244/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3375\n",
            "Epoch 245/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3289\n",
            "Epoch 246/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3235\n",
            "Epoch 247/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3164\n",
            "Epoch 248/250\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3107\n",
            "Epoch 249/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.3054\n",
            "Epoch 250/250\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.2994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37283a7c40>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = trainX[121:528]"
      ],
      "metadata": {
        "id": "O5xGLFasuDNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping tokens to words and vice-versa for both source and the target\n",
        "eng_word_to_token = eng_tokenizer.word_index\n",
        "eng_token_to_word = {token:word for word, token in eng_word_to_token.items()}\n",
        "\n",
        "hin_word_to_token = hin_tokenizer.word_index\n",
        "hin_token_to_word = {token:word for word, token in hin_word_to_token.items()}"
      ],
      "metadata": {
        "id": "8lKmg7RwuXGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, input, source_tokenizer, tar_token_to_word):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  input - a string in the source language\n",
        "  \"\"\"\n",
        "  tokenized = source_tokenizer.texts_to_sequences(test)\n",
        "  test = tf.keras.preprocessing.sequence.pad_sequences(tokenized, maxlen=8, padding='post')\n",
        "  prediction = model.predict(test)\n",
        "\n",
        "  output = [np.argmax(vector) for vector in prediction[0]]\n",
        "\n",
        "  output_list = []\n",
        "  for i in output:\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      output_list.append(tar_token_to_word[i])\n",
        "\n",
        "  output_sentence = ' '.join(output_list)\n",
        "  return output_sentence"
      ],
      "metadata": {
        "id": "S8AB1LX-zhDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}