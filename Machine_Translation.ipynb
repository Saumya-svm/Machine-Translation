{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBQU6qwn00-M"
      },
      "source": [
        "To-Do\n",
        "- [ ] Create Separate Functions for each task\n",
        "- [ ] Evaluation of our model\n",
        "- [ ] How to generalize the model?\n",
        "- [ ] More about preprocessing\n",
        "- [ ] Improve Handling of rare words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iilCsyZm8x0t",
        "outputId": "36c819f2-21c0-4674-a6bf-e832002c2a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras-preprocessing) (1.22.4)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qxYBi-0lCV09",
        "outputId": "9b950cde-9591-4b24-eeb8-e4be34e33049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text==2.11.0) (0.13.0)\n",
            "Collecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.4.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (67.6.1)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.53.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (16.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.32.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.2.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (6.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.1\n",
            "    Uninstalling tensorboard-2.12.1:\n",
            "      Successfully uninstalled tensorboard-2.12.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U tensorflow-text==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JUuV3MtyBIVk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, SimpleRNN, RepeatVector, TimeDistributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SEMBekqmA2Mn",
        "outputId": "9e60fdac-c784-4164-c026-f44ac5ed3a40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.11.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72epXW2bfogS"
      },
      "source": [
        "## English to Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtxLzvJZ_3mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a8e410-c321-4432-cd50-fd43786833cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uvC24IqtEqND"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  context = np.array([context for target, context in pairs])\n",
        "  target = np.array([target for target, context in pairs])\n",
        "\n",
        "  return target, context\n",
        "\n",
        "data = load_data(path_to_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "71w7mihMFf6Y"
      },
      "outputs": [],
      "source": [
        "inputs = np.array(data[0][:10000])\n",
        "outputs = np.array(data[1][:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "25GDof7ZH_cX"
      },
      "outputs": [],
      "source": [
        "# vectorizer = tf.keras.layers.TextVectorization(standardize=\"lower_and_strip_punctuation\", output_sequence_length=src)\n",
        "# text_dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "# vectorizer.adapt(text_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7gXUv2H4u_fC"
      },
      "outputs": [],
      "source": [
        "# # dump(vectorizer,open('drive/MyDrive/Machine Translation/vectroizer.pkl','wb'))\n",
        "# import pickle\n",
        "# pickle.dump({'config': vectorizer.get_config(),\n",
        "#              'weights': vectorizer.get_weights()}\n",
        "#             , open(\"drive/MyDrive/Machine Translation/vectorizer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "22735waO0qIl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# saved = pickle.load(open('drive/MyDrive/Machine Translation/vectorizer.pkl','rb'))\n",
        "# vectorizer = tf.keras.layers.TextVectorization.from_config(saved['config'])\n",
        "\n",
        "# vectorizer.adapt(tf.data.Dataset.from_tensor_slices(['random']))\n",
        "# vectorizer.set_weights(saved['weights'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zQFwEGKz7E0H"
      },
      "outputs": [],
      "source": [
        "# spanish_vectorizer = tf.keras.layers.TextVectorization(standardize=\"lower_and_strip_punctuation\", output_sequence_length=tar)\n",
        "# text_dataset = tf.data.Dataset.from_tensor_slices(outputs)\n",
        "# spanish_vectorizer.adapt(text_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qrqQQM-8Fldh"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': spanish_vectorizer.get_config(),\n",
        "#              'weights': spanish_vectorizer.get_weights()}\n",
        "#             , open(\"drive/MyDrive/Machine Translation/spanish_vectorizer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NG3zJ4o9FphM"
      },
      "outputs": [],
      "source": [
        "# saved = pickle.load(open('drive/MyDrive/Machine Translation/spanish_vectorizer.pkl','rb'))\n",
        "# spanish_vectorizer = tf.keras.layers.TextVectorization.from_config(saved['config'])\n",
        "\n",
        "# spanish_vectorizer.adapt(tf.data.Dataset.from_tensor_slices(['random']))\n",
        "# spanish_vectorizer.set_weights(saved['weights'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rkTGUT6RrsXd"
      },
      "outputs": [],
      "source": [
        "# train_X = vectorizer(inputs[indices])\n",
        "# train_Y = spanish_vectorizer(outputs[indices])\n",
        "# # model.fit(train_X, train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wSPUXMU7DW2u"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(source, target, num_words=5000, lower=True):\n",
        "  src_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,lower=lower)\n",
        "  tar_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,lower=lower)\n",
        "\n",
        "  src_tokenizer.fit_on_texts(source)\n",
        "  tar_tokenizer.fit_on_texts(target)\n",
        "  return src_tokenizer, tar_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hSY-Xcxc1tKX"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer,spa_tokenizer = create_tokenizer(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "isM-WspK3EE5"
      },
      "outputs": [],
      "source": [
        "def transform_data(source, target):\n",
        "  source = eng_tokenizer.texts_to_sequences(inputs)\n",
        "  source = tf.keras.preprocessing.sequence.pad_sequences(source, maxlen=8, padding='post')\n",
        "\n",
        "  target = spa_tokenizer.texts_to_sequences(outputs)\n",
        "  target = tf.keras.preprocessing.sequence.pad_sequences(target, maxlen=8, padding='post')\n",
        "  return source, target\n",
        "\n",
        "def split_data(source, target, train_split=0.8):\n",
        "  size = inputs.shape[0]\n",
        "  indices = np.random.randint(0, size, size=(int(size*train_split),))\n",
        "\n",
        "  trainX = source[indices]\n",
        "  trainY = target[indices]\n",
        "\n",
        "  mask = np.ones(size,bool)\n",
        "  mask[indices] = False\n",
        "  testX = source[mask]\n",
        "  testY = target[mask]\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "source, target = transform_data(inputs, outputs)\n",
        "trainX, trainY, testX, testY = split_data(source, target, 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yiyX-srWESTI"
      },
      "outputs": [],
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        " ylist = list()\n",
        " for sequence in sequences:\n",
        "  encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "  ylist.append(encoded)\n",
        " y = np.array(ylist)\n",
        " y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        " return y\n",
        "\n",
        "trainY = encode_output(trainY, len(spa_tokenizer.word_index)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e31oZY9SlTif"
      },
      "outputs": [],
      "source": [
        "tar_vocab = len(spa_tokenizer.word_index)+1\n",
        "src_vocab = len(eng_tokenizer.word_index)+1\n",
        "src = max(len(line.split()) for line in inputs)\n",
        "tar = max(len(line.split()) for line in outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jTb_utCwF0Y_"
      },
      "outputs": [],
      "source": [
        "layer1 = Embedding(src_vocab, 16, input_length=8)\n",
        "layer2 = LSTM(256)\n",
        "model = Sequential()\n",
        "model.add(layer1)\n",
        "model.add(layer2)\n",
        "\n",
        "# we want to replicate the context vector for each time step\n",
        "model.add(RepeatVector(8))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "# converting decoder output to our desired sequence format\n",
        "model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX[0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xKWOMspoVz",
        "outputId": "8e4616c5-2105-4c44-d0ff-81aa3b0e253e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 60, 190,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_tluu5rbaWo1"
      },
      "outputs": [],
      "source": [
        "# creating an encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, emb_vocab, emb_dim, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.emb_vocab = emb_vocab\n",
        "    self.emb_dim = emb_dim\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = Embedding(self.emb_vocab, self.emb_dim, input_length=8)\n",
        "    self.feature_extractor = tf.keras.layers.Bidirectional(merge_mode='sum', layer=tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self,input):\n",
        "    emb = self.embedding(input)\n",
        "    enc_output, enc_hidden, _ = self.feature_extractor(emb)\n",
        "\n",
        "    return enc_output, enc_hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YHBSszLRqjvB"
      },
      "outputs": [],
      "source": [
        "enc = Encoder(src_vocab,16,256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKVAIQLyYW5K",
        "outputId": "0dfb2753-ea1a-4b7a-e538-08a3776e450a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "trainX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4pT1cRdJq2Km"
      },
      "outputs": [],
      "source": [
        "enc_output, enc_hiddne = enc(trainX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDLsXARX87xu",
        "outputId": "bed82ceb-f231-4290-f097-670e6877980e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "np.array(enc_output[0,:,:]).reshape(8,-1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mhm-Yv5G35Mo"
      },
      "outputs": [],
      "source": [
        "w = tf.nn.softmax(Dense(1,activation='tanh')(np.array(enc_output[:,:,:]).reshape(-1,8,256)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4kHQZd48CXN",
        "outputId": "97de5579-9db8-4fb3-8861-e934214de88c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "np.array(w).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D88UDBg_cwDQ",
        "outputId": "23fb8e51-9eb1-44f1-9275-077573aed279"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8000, 8, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "enc_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2tFqIAWNagMW"
      },
      "outputs": [],
      "source": [
        "weights = np.array(tf.nn.softmax(np.array(w).reshape(-1,1,8)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1LQv7IPjt8kN"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units ):\n",
        "    super(Attention, self).__init__()\n",
        "    self.units = units\n",
        "    self.alignment = Dense(1, activation='tanh')\n",
        "    self.attention_weights = None\n",
        "\n",
        "  def call(self, enc_output, dec_hidden_state):\n",
        "    energy = self.alignment(enc_output)\n",
        "    self.attention_weights = tf.nn.softmax(tf.reshape(energy, [-1,1,8]))\n",
        "    context_vector = tf.matmul(self.attention_weights,enc_output)\n",
        "    return context_vector, self.attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "att = Attention(256)\n",
        "context, weights = att(enc_output, None)\n",
        "context.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3A2jS7XWgsw",
        "outputId": "4eb2a3fe-c2fe-42bb-976f-550508d381fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8000, 1, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FRlnhT78eCU9"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, units, tar_vocab):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.tar_vocab = tar_vocab\n",
        "    self.attention = Attention(units)\n",
        "    self.dec_cell = LSTM(256, return_sequences=True, return_state=True)\n",
        "    self.output_func = Dense(self.tar_vocab)\n",
        "    \n",
        "  def call(self, output, enc_output, hidden):\n",
        "    context, attention_weights = self.attention(enc_output, hidden)\n",
        "\n",
        "    # concat the output and the context vector\n",
        "    dec_input = tf.concat([context, output], axis=-1)\n",
        "    output, hidden_state,_ = self.dec_cell(dec_input)\n",
        "    output = self.output_func(output)\n",
        "\n",
        "    return output, hidden_state, attention_weights\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = trainY[:,0,:].reshape(-1,1,4961)"
      ],
      "metadata": {
        "id": "T1zlVNobD3hZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec = Decoder(256, tar_vocab)\n",
        "dec_output, hidden_state, attention_weights = dec(output,enc_output,None)"
      ],
      "metadata": {
        "id": "jJAUiMTasuPm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7vpT-44FauU",
        "outputId": "2aa6debc-768f-44d3-d85d-2499682de035"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8000, 1, 4961])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "D12r4Z2OpDZl"
      },
      "outputs": [],
      "source": [
        "def loss(y_true, y_pred):\n",
        "  loss_fun = tf.keras.losses.SparseCategoricalCorssentropy()\n",
        "  loss = loss_fun(y_true, y_pred)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "TtWUqwWxDMAN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec = Decoder(256, tar_vocab)"
      ],
      "metadata": {
        "id": "wqyATCmJ50hv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def training_step(input, y):\n",
        "  loss = 0\n",
        "  # enc = Encoder(tar_vocab, 16, 256)\n",
        "  # dec = Decoder(256, tar_vocab)\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = enc(input)\n",
        "    dec_hidden = enc_hidden\n",
        "    \n",
        "    output = tf.reshape(y[:,0,:], [-1,1,4961])\n",
        "    print('hello')\n",
        "    for i in range(1, y.shape[1]):\n",
        "      dec_output, hidden_state, attention_weights = dec(output, enc_output, dec_hidden)\n",
        "      loss += tf.keras.losses.CategoricalCrossentropy()(tf.reshape(y[:,i,:], (-1,1,4961)),dec_output)\n",
        "    batch_loss = loss/int(y.shape[1])\n",
        "    # print(loss, batch_loss)\n",
        "    # variables = enc.trainable_variables + dec.trainable_variables\n",
        "    variables = enc.trainable_variables + dec.trainable_variables + dec.attention.variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    # optimizer.build(variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "DclxN0DqfL9j"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, optimizer=tf.keras.optimizers.Adam(), trainX=trainX, trainY=trainY, epochs_per_step=32):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY))\n",
        "  length = trainX.shape[0]\n",
        "  batches = int(length/epochs_per_step)\n",
        "  losses = []\n",
        "  for epoch in range(50):\n",
        "    start = time.time()\n",
        "    epoch_loss = 0\n",
        "    for batch in range(batches):\n",
        "      indices_start = batch*epochs_per_step\n",
        "      indices_end = indices_start+epochs_per_step\n",
        "      input = trainX[indices_start:indices_end]\n",
        "      tar = trainY[indices_start:indices_end]\n",
        "      # print(input.shape)\n",
        "      # print(tar.shape)\n",
        "      # break\n",
        "      batch_loss = training_step(input, tar)\n",
        "      epoch_loss += batch_loss\n",
        "    \n",
        "    losses.append(epoch_loss/batches)\n",
        "    print(f\"Epoch {epoch+1} loss {epoch_loss/batches}\")\n",
        "    print(time.time()-start)\n",
        "  return losses\n",
        "    "
      ],
      "metadata": {
        "id": "E8MtSVnsoUT1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "losses = train(10)\n",
        "end = time.time()\n",
        "end-start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZdhh4tFZ0ph",
        "outputId": "906eefab-6e08-49bc-ae5e-c1e2bb7eed81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss 10.791194915771484\n",
            "6.24383282661438\n",
            "Epoch 2 loss 10.864636421203613\n",
            "5.6867029666900635\n",
            "Epoch 3 loss 10.297652244567871\n",
            "5.833936452865601\n",
            "Epoch 4 loss 10.426053047180176\n",
            "6.16764760017395\n",
            "Epoch 5 loss 11.34715747833252\n",
            "5.648762941360474\n",
            "Epoch 6 loss 10.3530855178833\n",
            "6.228830099105835\n",
            "Epoch 7 loss 9.13405704498291\n",
            "5.696537494659424\n",
            "Epoch 8 loss 8.522539138793945\n",
            "5.733081340789795\n",
            "Epoch 9 loss 8.922370910644531\n",
            "6.1769609451293945\n",
            "Epoch 10 loss 8.295306205749512\n",
            "5.596759557723999\n",
            "Epoch 11 loss 8.287266731262207\n",
            "6.001916408538818\n",
            "Epoch 12 loss 7.410953998565674\n",
            "5.672987461090088\n",
            "Epoch 13 loss 8.080018043518066\n",
            "6.408250093460083\n",
            "Epoch 14 loss 7.115900039672852\n",
            "6.0370728969573975\n",
            "Epoch 15 loss 7.766013145446777\n",
            "5.616457939147949\n",
            "Epoch 16 loss 6.580559730529785\n",
            "6.092764616012573\n",
            "Epoch 17 loss 6.037989139556885\n",
            "5.673971891403198\n",
            "Epoch 18 loss 5.450923442840576\n",
            "5.634390115737915\n",
            "Epoch 19 loss 4.9709367752075195\n",
            "6.062699794769287\n",
            "Epoch 20 loss 6.567409038543701\n",
            "5.578620672225952\n",
            "Epoch 21 loss 5.592087745666504\n",
            "5.956203937530518\n",
            "Epoch 22 loss 5.150505065917969\n",
            "5.769207954406738\n",
            "Epoch 23 loss 5.158941745758057\n",
            "5.651856899261475\n",
            "Epoch 24 loss 4.929286003112793\n",
            "6.093465566635132\n",
            "Epoch 25 loss 4.669802665710449\n",
            "5.569216251373291\n",
            "Epoch 26 loss 4.947022914886475\n",
            "5.887066841125488\n",
            "Epoch 27 loss 4.68784761428833\n",
            "5.882795572280884\n",
            "Epoch 28 loss 4.501086711883545\n",
            "5.560081958770752\n",
            "Epoch 29 loss 4.333101272583008\n",
            "6.114472150802612\n",
            "Epoch 30 loss 4.832474231719971\n",
            "5.565587282180786\n",
            "Epoch 31 loss 4.605799198150635\n",
            "5.7325708866119385\n",
            "Epoch 32 loss 4.593608856201172\n",
            "5.951630115509033\n",
            "Epoch 33 loss 4.799194812774658\n",
            "5.618165969848633\n",
            "Epoch 34 loss 4.509350776672363\n",
            "6.150691509246826\n",
            "Epoch 35 loss 4.907880783081055\n",
            "5.646787881851196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses"
      ],
      "metadata": {
        "id": "yjW42L1YwOtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class translator(tf.keras.Model):\n",
        "  def __init__(self, emb_vocab, emb_dim, units, tar_vocab):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(emb_vocab, emb_dim, units)\n",
        "    self.decoder = Encoder(units, tar_vocab)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    x = self.encoder(inputs)\n",
        "    x = self.decoder(outputs)"
      ],
      "metadata": {
        "id": "2LoE1TVY8PE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM2cr3Hrr275",
        "outputId": "c4140fb0-858b-4215-8bdc-9725f71aad29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf3QbyPwdvkI",
        "outputId": "c4a9a9f9-eda8-405f-92e4-27ee319295d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 8, 16)             37888     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256)               279552    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 8, 256)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 8, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 8, 4961)          1274977   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,117,729\n",
            "Trainable params: 2,117,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPAQfPaNMxug",
        "outputId": "92f12e05-78c8-48bf-e20c-6d2a35e3eb41"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 8, 4961)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([[[0,1,0],[1,0,0]],[[1,0,0],[0,0,1]]])\n",
        "y_pred = np.array([[[0.01,0.95,0.04],[0.97,0.02,0.01]],[[0.96,0.02,0.02],[0.02,0.03,0.95]]])\n",
        "y_true.shape, y_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJd5uLeNW7f",
        "outputId": "465bef3e-dc6b-47b2-ad7c-398695fd4f32"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 2, 3), (2, 2, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_true = np.array([[[0, 1, 0], [0, 0, 1]]])\n",
        "# print(y_true.shape)\n",
        "# y_pred = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1]]]\n",
        "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "cce(y_true[1,:,:], y_pred[1,:,:]).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slzlZMqmNTfy",
        "outputId": "6138573e-3213-44e7-f6d3-ce4379f6cd1b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04605764445390287"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyIGyKKN5EcZ",
        "outputId": "7934aa3e-68d7-4028-d441-100c7c6d9519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "125/125 [==============================] - 11s 49ms/step - loss: 3.4223\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.5998\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 2.5356\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.4661\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 2.4098\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 2.3634\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 2.3301\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 2.2902\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.2556\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.2216\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 2.1896\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 2.1626\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 2.1279\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 2.0844\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 2.0391\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 1.9945\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.9529\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 1.9097\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.8731\n",
            "Epoch 20/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.8339\n",
            "Epoch 21/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.8003\n",
            "Epoch 22/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.7598\n",
            "Epoch 23/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 1.7269\n",
            "Epoch 24/30\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 1.6919\n",
            "Epoch 25/30\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 1.6469\n",
            "Epoch 26/30\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 1.6057\n",
            "Epoch 27/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.5697\n",
            "Epoch 28/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.5387\n",
            "Epoch 29/30\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 1.5017\n",
            "Epoch 30/30\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 1.4697\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32af328dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "model.fit(trainX, trainY, epochs=30, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = [layer.output for layer in model.layers]"
      ],
      "metadata": {
        "id": "okkaJbmvySK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgyGx7dNkF1D",
        "outputId": "66a64add-60e7-4323-9bdb-d8010d30b4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 9s 31ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(trainX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO3kmksXuwH3"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY[0, :,:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzgPAGK0LLb9",
        "outputId": "53c761e9-9b38-4860-9b74-0e5ace73ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 4961)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aq8jC7zC-2z"
      },
      "outputs": [],
      "source": [
        "def inference(model, input, source_tokenizer, tar_token_to_word):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  input - a string in the source language\n",
        "  \"\"\"\n",
        "  # tokenized = source_tokenizer.texts_to_sequences(input)\n",
        "  # test = tf.keras.preprocessing.sequence.pad_sequences(tokenized, maxlen=8, padding='post')\n",
        "  prediction = model.predict(input)\n",
        "\n",
        "  output = [np.argmax(vector) for vector in prediction[0]]\n",
        "\n",
        "  output_list = []\n",
        "  for i in output:\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      output_list.append(tar_token_to_word[i])\n",
        "\n",
        "  output_sentence = ' '.join(output_list)\n",
        "  return output_sentence, output\n",
        "\n",
        "eng_word_to_token = eng_tokenizer.word_index\n",
        "eng_token_to_word = {token:word for word, token in eng_word_to_token.items()}\n",
        "\n",
        "spa_word_to_token = spa_tokenizer.word_index\n",
        "spa_token_to_word = {token:word for word, token in spa_word_to_token.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yrPlRfpbr76"
      },
      "outputs": [],
      "source": [
        "sent, output = inference(model, trainX[123:134], eng_tokenizer, spa_token_to_word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [layer for layer in model.layers]"
      ],
      "metadata": {
        "id": "NOGqZkhe8J1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers[1]()"
      ],
      "metadata": {
        "id": "Acz2QjZ79HP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[layer.input for layer in model.layers]"
      ],
      "metadata": {
        "id": "wESZnHeF9Sg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_output.shape"
      ],
      "metadata": {
        "id": "V6LA06KgqrlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = np.array(layers[0](np.array(trainX[0:12])))\n",
        "layers[-1](np.array(enc_output[0,0,:]).reshape(1,1,-1)).shape"
      ],
      "metadata": {
        "id": "2wURiBcm8MlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(enc_output[0,0,:]).shape"
      ],
      "metadata": {
        "id": "sZj7wuIQuK_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = LSTM(128, input_shape=(8,256), return_sequences=True, return_state=True)\n",
        "layer(np.array(enc_output[0,0,:]).reshape(1,1,256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuXIYyt8tbwv",
        "outputId": "c617924a-f18a-40fd-8da9-40a7129078af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1, 1, 128), dtype=float32, numpy=\n",
              " array([[[-1.4551018e-03, -8.4214687e-04,  9.9801912e-04,  4.0674073e-04,\n",
              "           6.3562952e-04,  1.3461038e-03, -2.8510811e-04, -2.5650763e-04,\n",
              "          -1.5970368e-04,  1.4687947e-03,  2.4260647e-04, -4.5335016e-04,\n",
              "          -4.2363498e-04,  1.6707971e-05,  3.9971250e-04, -7.5283658e-04,\n",
              "           1.7399113e-03,  8.7354629e-04, -7.6641125e-04, -4.5693957e-04,\n",
              "           3.0355118e-04,  1.4359169e-03,  1.4759449e-04, -7.6561468e-05,\n",
              "           3.3676540e-04,  2.0027789e-04,  4.0466568e-04,  5.7189236e-04,\n",
              "          -6.0488546e-04, -1.1757636e-04, -1.5828786e-04, -1.3522982e-03,\n",
              "          -1.1456797e-03, -6.0270802e-04, -1.8989336e-03,  7.4525026e-04,\n",
              "          -1.3446304e-04, -7.1785349e-04,  1.3909987e-03,  6.1527395e-04,\n",
              "           1.6450656e-04, -6.6929171e-04,  3.5361017e-04, -1.4782239e-03,\n",
              "          -3.5549681e-05,  1.0267527e-04, -1.1837718e-03, -7.3577208e-04,\n",
              "           5.3062837e-04,  4.1731278e-04,  1.0159778e-03, -2.6139923e-04,\n",
              "           1.6440895e-04, -1.0628072e-03, -1.1812107e-03,  8.4981875e-04,\n",
              "          -7.0599595e-04,  5.9805921e-04, -3.7799732e-04,  7.6637603e-04,\n",
              "           2.8034218e-04,  2.7178909e-04, -8.5909106e-04, -1.3247264e-03,\n",
              "           7.5570761e-06,  1.6790719e-03,  6.3504541e-04, -1.5276521e-03,\n",
              "          -9.3605125e-04,  7.9767150e-04, -2.9410754e-04, -2.9085120e-04,\n",
              "          -3.6169522e-04, -1.5768412e-03, -1.3823516e-04, -8.2694466e-04,\n",
              "          -4.4204708e-04, -3.2313800e-04,  1.8414218e-03,  1.6166955e-03,\n",
              "          -9.2950046e-05, -6.0314307e-04,  7.8750099e-04, -8.5126451e-04,\n",
              "           1.1012679e-03,  8.3252240e-04, -8.1974361e-04,  5.0250581e-04,\n",
              "           2.5529850e-03, -3.3297396e-04,  3.5877452e-05,  6.2945118e-04,\n",
              "           1.7344381e-04,  3.7968403e-04,  4.6327399e-04, -2.1332753e-06,\n",
              "           2.8731339e-04,  7.9944031e-04,  4.5951991e-04,  1.0708963e-03,\n",
              "          -3.5013407e-04, -2.1157967e-04,  1.0851633e-04,  1.5569510e-04,\n",
              "           4.7673928e-04,  6.0613343e-04,  9.6832555e-05,  1.3866044e-03,\n",
              "           7.8919096e-05,  2.9093950e-04, -5.7153619e-04,  1.2450536e-04,\n",
              "          -5.8251771e-05, -1.5419514e-03, -8.6461613e-04, -1.6300855e-04,\n",
              "           3.9029910e-05, -6.6651759e-04, -4.3477415e-04, -6.3714286e-04,\n",
              "          -3.1386374e-04,  5.0357929e-05,  2.6304869e-04,  1.9769205e-04,\n",
              "           4.7214155e-04, -1.4325618e-04,  4.9537841e-05, -1.8239740e-03]]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
              " array([[-1.4551018e-03, -8.4214687e-04,  9.9801912e-04,  4.0674073e-04,\n",
              "          6.3562952e-04,  1.3461038e-03, -2.8510811e-04, -2.5650763e-04,\n",
              "         -1.5970368e-04,  1.4687947e-03,  2.4260647e-04, -4.5335016e-04,\n",
              "         -4.2363498e-04,  1.6707971e-05,  3.9971250e-04, -7.5283658e-04,\n",
              "          1.7399113e-03,  8.7354629e-04, -7.6641125e-04, -4.5693957e-04,\n",
              "          3.0355118e-04,  1.4359169e-03,  1.4759449e-04, -7.6561468e-05,\n",
              "          3.3676540e-04,  2.0027789e-04,  4.0466568e-04,  5.7189236e-04,\n",
              "         -6.0488546e-04, -1.1757636e-04, -1.5828786e-04, -1.3522982e-03,\n",
              "         -1.1456797e-03, -6.0270802e-04, -1.8989336e-03,  7.4525026e-04,\n",
              "         -1.3446304e-04, -7.1785349e-04,  1.3909987e-03,  6.1527395e-04,\n",
              "          1.6450656e-04, -6.6929171e-04,  3.5361017e-04, -1.4782239e-03,\n",
              "         -3.5549681e-05,  1.0267527e-04, -1.1837718e-03, -7.3577208e-04,\n",
              "          5.3062837e-04,  4.1731278e-04,  1.0159778e-03, -2.6139923e-04,\n",
              "          1.6440895e-04, -1.0628072e-03, -1.1812107e-03,  8.4981875e-04,\n",
              "         -7.0599595e-04,  5.9805921e-04, -3.7799732e-04,  7.6637603e-04,\n",
              "          2.8034218e-04,  2.7178909e-04, -8.5909106e-04, -1.3247264e-03,\n",
              "          7.5570761e-06,  1.6790719e-03,  6.3504541e-04, -1.5276521e-03,\n",
              "         -9.3605125e-04,  7.9767150e-04, -2.9410754e-04, -2.9085120e-04,\n",
              "         -3.6169522e-04, -1.5768412e-03, -1.3823516e-04, -8.2694466e-04,\n",
              "         -4.4204708e-04, -3.2313800e-04,  1.8414218e-03,  1.6166955e-03,\n",
              "         -9.2950046e-05, -6.0314307e-04,  7.8750099e-04, -8.5126451e-04,\n",
              "          1.1012679e-03,  8.3252240e-04, -8.1974361e-04,  5.0250581e-04,\n",
              "          2.5529850e-03, -3.3297396e-04,  3.5877452e-05,  6.2945118e-04,\n",
              "          1.7344381e-04,  3.7968403e-04,  4.6327399e-04, -2.1332753e-06,\n",
              "          2.8731339e-04,  7.9944031e-04,  4.5951991e-04,  1.0708963e-03,\n",
              "         -3.5013407e-04, -2.1157967e-04,  1.0851633e-04,  1.5569510e-04,\n",
              "          4.7673928e-04,  6.0613343e-04,  9.6832555e-05,  1.3866044e-03,\n",
              "          7.8919096e-05,  2.9093950e-04, -5.7153619e-04,  1.2450536e-04,\n",
              "         -5.8251771e-05, -1.5419514e-03, -8.6461613e-04, -1.6300855e-04,\n",
              "          3.9029910e-05, -6.6651759e-04, -4.3477415e-04, -6.3714286e-04,\n",
              "         -3.1386374e-04,  5.0357929e-05,  2.6304869e-04,  1.9769205e-04,\n",
              "          4.7214155e-04, -1.4325618e-04,  4.9537841e-05, -1.8239740e-03]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
              " array([[-2.9117621e-03, -1.6819033e-03,  1.9981428e-03,  8.1466604e-04,\n",
              "          1.2696937e-03,  2.6923257e-03, -5.7081599e-04, -5.1287439e-04,\n",
              "         -3.1898066e-04,  2.9316435e-03,  4.8495160e-04, -9.0940099e-04,\n",
              "         -8.4710761e-04,  3.3425680e-05,  7.9783937e-04, -1.5056808e-03,\n",
              "          3.4895299e-03,  1.7498729e-03, -1.5303876e-03, -9.1327995e-04,\n",
              "          6.0977013e-04,  2.8709928e-03,  2.9484089e-04, -1.5309347e-04,\n",
              "          6.7261147e-04,  3.9978305e-04,  8.0874067e-04,  1.1454255e-03,\n",
              "         -1.2094560e-03, -2.3504460e-04, -3.1590427e-04, -2.7100903e-03,\n",
              "         -2.2911208e-03, -1.2055058e-03, -3.8030692e-03,  1.4959092e-03,\n",
              "         -2.6843761e-04, -1.4378414e-03,  2.7829898e-03,  1.2311513e-03,\n",
              "          3.2802732e-04, -1.3378625e-03,  7.0739933e-04, -2.9579164e-03,\n",
              "         -7.0945331e-05,  2.0559554e-04, -2.3663177e-03, -1.4742680e-03,\n",
              "          1.0638154e-03,  8.3686929e-04,  2.0307668e-03, -5.2308111e-04,\n",
              "          3.2948997e-04, -2.1309024e-03, -2.3694588e-03,  1.6985482e-03,\n",
              "         -1.4118536e-03,  1.1966553e-03, -7.5482816e-04,  1.5299424e-03,\n",
              "          5.6153419e-04,  5.4261024e-04, -1.7195243e-03, -2.6463654e-03,\n",
              "          1.5123539e-05,  3.3619495e-03,  1.2703326e-03, -3.0487734e-03,\n",
              "         -1.8737556e-03,  1.5924338e-03, -5.8845681e-04, -5.7989982e-04,\n",
              "         -7.2356907e-04, -3.1509493e-03, -2.7713968e-04, -1.6552367e-03,\n",
              "         -8.8715157e-04, -6.4537750e-04,  3.6878893e-03,  3.2313219e-03,\n",
              "         -1.8610670e-04, -1.2088544e-03,  1.5727783e-03, -1.7052281e-03,\n",
              "          2.2011176e-03,  1.6647797e-03, -1.6351485e-03,  1.0064201e-03,\n",
              "          5.0906693e-03, -6.6589128e-04,  7.1709255e-05,  1.2609459e-03,\n",
              "          3.4727028e-04,  7.5763342e-04,  9.2741993e-04, -4.2710744e-06,\n",
              "          5.7288189e-04,  1.5999079e-03,  9.2071120e-04,  2.1510399e-03,\n",
              "         -6.9894770e-04, -4.2308454e-04,  2.1702943e-04,  3.1170697e-04,\n",
              "          9.5231464e-04,  1.2100959e-03,  1.9366453e-04,  2.7667454e-03,\n",
              "          1.5777838e-04,  5.8034970e-04, -1.1416172e-03,  2.4879968e-04,\n",
              "         -1.1647512e-04, -3.0887260e-03, -1.7301773e-03, -3.2574212e-04,\n",
              "          7.7914439e-05, -1.3331131e-03, -8.7140460e-04, -1.2741035e-03,\n",
              "         -6.2828721e-04,  1.0060866e-04,  5.2470219e-04,  3.9513709e-04,\n",
              "          9.4091834e-04, -2.8625800e-04,  9.9148572e-05, -3.6487391e-03]],\n",
              "       dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfCz8EmQfhgN"
      },
      "source": [
        "## Eng to Hin (Incomplete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQVzihcHevg1"
      },
      "outputs": [],
      "source": [
        "with open('hin.txt') as f:\n",
        "    lines = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-TPwGoUoGOz"
      },
      "outputs": [],
      "source": [
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fesx1GluoS6n"
      },
      "outputs": [],
      "source": [
        "def prepare_data(lines):\n",
        "  inputs = []\n",
        "  outputs = []\n",
        "\n",
        "  for i in range(len(lines)):\n",
        "    src, target, _ = lines[i].split(\"\\t\")\n",
        "    inputs.append(src)\n",
        "    outputs.append(target)\n",
        "  \n",
        "  inputs = np.array(inputs)\n",
        "  outputs = np.array(outputs)\n",
        "  return inputs, outputs\n",
        "\n",
        "inputs, outputs = prepare_data(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXrcmh1ioykU"
      },
      "outputs": [],
      "source": [
        "inputs.shape, outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfUtn7k9q_Xq"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,lower=True)\n",
        "hin_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,lower=True)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(inputs)\n",
        "hin_tokenizer.fit_on_texts(outputs)\n",
        "\n",
        "trainX = eng_tokenizer.texts_to_sequences(inputs)\n",
        "trainX = tf.keras.preprocessing.sequence.pad_sequences(trainX, maxlen=8, padding='post')\n",
        "\n",
        "trainY = hin_tokenizer.texts_to_sequences(outputs)\n",
        "trainY = tf.keras.preprocessing.sequence.pad_sequences(trainY, maxlen=8, padding='post')\n",
        "\n",
        "def encode_output(sequences, vocab_size):\n",
        " ylist = list()\n",
        " for sequence in sequences:\n",
        "  encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "  ylist.append(encoded)\n",
        " y = np.array(ylist)\n",
        " y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        " return y\n",
        "\n",
        "trainY = encode_output(trainY[:9000], len(hin_tokenizer.word_index)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj0VzOpStLf-"
      },
      "outputs": [],
      "source": [
        "len(hin_tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIrHxw7AsNsF"
      },
      "outputs": [],
      "source": [
        "tar_vocab = len(hin_tokenizer.word_index)+1\n",
        "src_vocab = len(eng_tokenizer.word_index)+1\n",
        "src = max(len(line.split()) for line in inputs)\n",
        "tar = max(len(line.split()) for line in outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Naul2XIsl5v"
      },
      "outputs": [],
      "source": [
        "layer1 = Embedding(src_vocab, 16, input_length=8)\n",
        "layer2 = LSTM(256)\n",
        "model = Sequential()\n",
        "model.add(layer1)\n",
        "model.add(layer2)\n",
        "\n",
        "# we want to replicate the context vector for each time step\n",
        "model.add(RepeatVector(8))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "# converting decoder output to our desired sequence format\n",
        "model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPyy-C9vEUes"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGZslmep0CUE"
      },
      "outputs": [],
      "source": [
        "trainX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD3pmD6OdmLf"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons==0.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf4MriT5oVTY"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM8GKc6tDY5o"
      },
      "outputs": [],
      "source": [
        "bi_model = Sequential()\n",
        "bi_model.add(Embedding(src_vocab, 16, input_length=8))\n",
        "bi_model.add(tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True), input_shape=(8,16)))\n",
        "# bi_model.add(tf.keras.layers.Attention(256))\n",
        "bi_model.add(LSTM(256, return_sequences=True))\n",
        "bi_model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "bi_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "bi_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_nQjPDQ8s5z"
      },
      "outputs": [],
      "source": [
        "trainY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxSG_i6l5DbW"
      },
      "outputs": [],
      "source": [
        "bi_model.fit(trainX, trainY, epochs=250, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxw87ktU39xQ"
      },
      "outputs": [],
      "source": [
        "predictions = bi_model.predict(trainX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQkzAIgS4MYq"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vSD0kZj9Rbn"
      },
      "outputs": [],
      "source": [
        "[np.argmax(vector) for vector in predictions[14]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrscudPjsoBN"
      },
      "outputs": [],
      "source": [
        "model.fit(trainX, trainY, epochs=250, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5xGLFasuDNI"
      },
      "outputs": [],
      "source": [
        "input = trainX[121:528]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lKmg7RwuXGS"
      },
      "outputs": [],
      "source": [
        "# mapping tokens to words and vice-versa for both source and the target\n",
        "eng_word_to_token = eng_tokenizer.word_index\n",
        "eng_token_to_word = {token:word for word, token in eng_word_to_token.items()}\n",
        "\n",
        "hin_word_to_token = hin_tokenizer.word_index\n",
        "hin_token_to_word = {token:word for word, token in hin_word_to_token.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8AB1LX-zhDx"
      },
      "outputs": [],
      "source": [
        "def inference(model, input, source_tokenizer, tar_token_to_word):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  input - a string in the source language\n",
        "  \"\"\"\n",
        "  tokenized = source_tokenizer.texts_to_sequences(input)\n",
        "  input = tf.keras.preprocessing.sequence.pad_sequences(tokenized, maxlen=8, padding='post')\n",
        "  prediction = model.predict(input)\n",
        "\n",
        "  # print(input)\n",
        "  input_list = []\n",
        "  for i in input[0]:\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      input_list.append(eng_token_to_word[i])\n",
        "  print(\"Input: \", ' '.join(input_list))\n",
        "\n",
        "  output = [np.argmax(vector) for vector in prediction[0]]\n",
        "\n",
        "  output_list = []\n",
        "  for i in output:\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      output_list.append(tar_token_to_word[i])\n",
        "\n",
        "  output_sentence = ' '.join(output_list)\n",
        "  return output_sentence\n",
        "\n",
        "input = [\"how you doing\"]\n",
        "inference(bi_model, input, eng_tokenizer, hin_token_to_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFW4BuX8NaqY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}